"""
CueNote Core - Web Extractor
URLì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ëŠ” ëª¨ë“ˆ
"""
import re
from urllib.parse import urljoin, urlparse
from typing import Optional

import httpx
import trafilatura
from trafilatura.settings import use_config

from .config import logger

# trafilatura ì„¤ì •
_traf_config = use_config()
_traf_config.set("DEFAULT", "MIN_OUTPUT_SIZE", "200")
_traf_config.set("DEFAULT", "MIN_EXTRACTED_SIZE", "100")


async def fetch_url(url: str, timeout: float = 15.0) -> str:
    """
    URLì—ì„œ HTMLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Args:
        url: ê°€ì ¸ì˜¬ URL
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)

    Returns:
        HTML ë¬¸ìì—´

    Raises:
        ValueError: ì˜ëª»ëœ URL
        httpx.HTTPError: HTTP ìš”ì²­ ì‹¤íŒ¨
    """
    # URL ìœ íš¨ì„± ê²€ì‚¬
    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        raise ValueError(f"ì˜¬ë°”ë¥¸ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”: {url}")

    if parsed.scheme not in ("http", "https"):
        raise ValueError(f"HTTP/HTTPS URLë§Œ ì§€ì›í•©ë‹ˆë‹¤: {url}")

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    }

    async with httpx.AsyncClient(
        follow_redirects=True,
        timeout=timeout,
        verify=False,  # SSL ì¸ì¦ì„œ ê²€ì¦ ìƒëµ (ì¼ë¶€ ì‚¬ì´íŠ¸ í˜¸í™˜)
    ) as client:
        response = await client.get(url, headers=headers)
        response.raise_for_status()
        return response.text


def extract_content(html: str, url: str) -> dict:
    """
    HTMLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì œëª©, ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        html: HTML ë¬¸ìì—´
        url: ì›ë³¸ URL (ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ ë³€í™˜ìš©)

    Returns:
        {
            "title": str,
            "text": str,
            "images": list[str],
        }
    """
    # trafilaturaë¡œ ë³¸ë¬¸ ì¶”ì¶œ
    text = trafilatura.extract(
        html,
        include_images=True,
        include_links=True,
        include_tables=True,
        output_format="txt",
        config=_traf_config,
    )

    if not text:
        # fallback: trafilatura ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¶”ì¶œ
        text = trafilatura.extract(
            html,
            include_images=False,
            include_links=False,
            config=_traf_config,
        )

    # ì œëª© ì¶”ì¶œ
    title = _extract_title(html)

    return {
        "title": title or "",
        "text": text or "",
        "images": [],
    }


def _extract_title(html: str) -> str:
    """HTMLì—ì„œ ì œëª©(title íƒœê·¸)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.search(r"<title[^>]*>(.*?)</title>", html, re.IGNORECASE | re.DOTALL)
    if match:
        title = match.group(1).strip()
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        title = title.replace("&amp;", "&")
        title = title.replace("&lt;", "<")
        title = title.replace("&gt;", ">")
        title = title.replace("&quot;", '"')
        title = title.replace("&#39;", "'")
        return title
    return ""


def _extract_image_urls(html: str, base_url: str, max_images: int = 20) -> list[str]:
    """
    HTMLì—ì„œ ì£¼ìš” ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì•„ì´ì½˜, ë¡œê³  ë“± ì‘ì€ ì´ë¯¸ì§€ëŠ” í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    img_pattern = re.compile(
        r'<img[^>]+src=["\']([^"\']+)["\']',
        re.IGNORECASE,
    )

    seen = set()
    images = []

    for match in img_pattern.finditer(html):
        src = match.group(1).strip()

        # ë°ì´í„° URI ìŠ¤í‚µ
        if src.startswith("data:"):
            continue

        # ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ
        abs_url = urljoin(base_url, src)

        # ì¤‘ë³µ ì²´í¬
        if abs_url in seen:
            continue
        seen.add(abs_url)

        # ì•„ì´ì½˜/ë¡œê³  í•„í„°ë§ (íŒŒì¼ëª… ê¸°ë°˜)
        lower = abs_url.lower()
        skip_patterns = [
            "favicon", "icon", "logo", "badge", "avatar",
            "pixel", "tracking", "spacer", "blank",
            "1x1", "sprite", ".svg",
        ]
        if any(p in lower for p in skip_patterns):
            continue

        images.append(abs_url)

        if len(images) >= max_images:
            break

    return images


def build_markdown(
    title: str,
    text: str,
    images: list[str],
    source_url: str,
) -> str:
    """
    ì¶”ì¶œëœ ì½˜í…ì¸ ë¥¼ ë§ˆí¬ë‹¤ìš´ ì´ˆì•ˆìœ¼ë¡œ ì¡°í•©í•©ë‹ˆë‹¤.

    Args:
        title: í˜ì´ì§€ ì œëª©
        text: ë³¸ë¬¸ í…ìŠ¤íŠ¸
        images: ì´ë¯¸ì§€ URL ëª©ë¡
        source_url: ì›ë³¸ URL

    Returns:
        ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´
    """
    parts = []

    # ì œëª©
    if title:
        parts.append(f"# {title}\n")

    # ì¶œì²˜
    parts.append(f"> ğŸ“ ì¶œì²˜: [{source_url}]({source_url})\n")

    # ë³¸ë¬¸
    if text:
        parts.append(text)

    return "\n".join(parts)
